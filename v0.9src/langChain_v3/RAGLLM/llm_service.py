# langChain_v3/RAGLLM/llm_service.py

# from .llm_runtime import get_llm


# SYSTEM_PROMPT_DEFAULT = (
#     "ë‹¹ì‹ ì€ ê°€ì²œëŒ€í•™êµ í•™ì‚¬Â·í–‰ì • ì •ë³´ë¥¼ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
#     "ì§ˆë¬¸ì— ëŒ€í•´ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
#     "ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  'í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”.\n"
# )


# def generate_answer(
#     user_prompt: str,
#     system_prompt: str = SYSTEM_PROMPT_DEFAULT,
# ) -> str:
#     """
#     ì„œë²„/CLI/RAGì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” LLM í˜¸ì¶œ í•¨ìˆ˜
#     """
#     llm = get_llm()          # ğŸ”¥ ì—¬ê¸°ì„œ ë‹¨ í•œ ë²ˆë§Œ ë¡œë“œë¨
#     return llm.generate(system_prompt, user_prompt)

from langChain_v3.RAGLLM.gpt_api_llm import GPTAPILLM

_llm = None

def get_llm():
    global _llm
    if _llm is None:
        _llm = GPTAPILLM()
    return _llm

def generate_answer(system_prompt, user_prompt):
    llm = get_llm()
    return llm.generate(system_prompt, user_prompt)