# langChains_v3/rag.py
# FAISSì—ì„œ top-k chunk ê²€ìƒ‰ í›„, ê²€ìƒ‰ëœ ê²°ê³¼ì— ëŒ€í•´ ë¬¸ë§¥ í™•ì¥ëœ ì²­í¬ë¥¼ ì „ì†¡
# rag_engine/rag.py
import os
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any, Callable, Iterable, Optional

from langChain_v3.vectorstore import load_vectorstore, DEFAULT_INDEX_DIR
from langChain_v3.db import get_connection

# Remove duplicate overlap between adjacent chunks.
def _trim_overlap(prev_text: str, next_text: str, max_overlap_chars: int = 1000) -> str:
    if not prev_text or not next_text:
        return next_text

    max_len = min(len(prev_text), len(next_text), max_overlap_chars)
    for size in range(max_len, 0, -1):
        if prev_text.endswith(next_text[:size]):
            return next_text[size:]

    return next_text


def _merge_chunks_without_overlap(
    texts: List[str], max_overlap_chars: int = 1000
) -> str:
    merged: List[str] = []
    prev_text = ""

    for text in texts:
        if not text:
            continue
        if not merged:
            merged.append(text)
            prev_text = text
            continue

        trimmed = _trim_overlap(prev_text, text, max_overlap_chars)
        if trimmed:
            merged.append(trimmed)
        prev_text = text

    return "\n\n".join(merged)


def _default_rerank_device() -> str:
    forced = os.getenv("RERANK_DEVICE", "").strip().lower()
    if forced:
        return forced
    try:
        import torch  # type: ignore

        return "cuda" if torch.cuda.is_available() else "cpu"
    except Exception:
        return "cpu"


def _attach_source(
    rows: Iterable[Dict[str, Any]], source: str
) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for idx, row in enumerate(rows, start=1):
        tagged = dict(row)
        tagged.setdefault("source", source)
        tagged.setdefault("source_rank", idx)
        out.append(tagged)
    return out


def _normalize_web_results(rows: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
    normalized: List[Dict[str, Any]] = []
    for idx, row in enumerate(rows, start=1):
        if not isinstance(row, dict):
            continue
        title = row.get("title") or row.get("name") or ""
        url = row.get("url") or row.get("link") or ""
        snippet = row.get("snippet") or row.get("content") or row.get("text") or ""
        if not (title or url or snippet):
            continue
        item: Dict[str, Any] = {
            "title": title,
            "url": url,
            "snippet": snippet,
            "context_text": snippet,
            "source": "web",
            "source_rank": idx,
        }
        if row.get("score") is not None:
            try:
                item["score"] = float(row["score"])
            except Exception:
                pass
        normalized.append(item)
    return normalized


def _dedupe_results(rows: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    deduped: List[Dict[str, Any]] = []
    for row in rows:
        source = row.get("source")
        url = (row.get("url") or "").strip().lower()
        if url:
            key = ("url", source, url)
        else:
            key = (
                "meta",
                source,
                row.get("meta_id"),
                row.get("chunk_id"),
                row.get("title"),
            )
        if key in seen:
            continue
        seen.add(key)
        deduped.append(row)
    return deduped


def _collect_web_results(
    query: str, web_k: int, web_search_fn: Callable[..., List[Dict[str, Any]]]
) -> List[Dict[str, Any]]:
    try:
        return web_search_fn(query, k=web_k)
    except TypeError:
        return web_search_fn(query, web_k)
    except Exception:
        return []

#ë¬¸ë§¥ í™•ì¥ í•¨ìˆ˜
def expand_context(cur, meta_id: str, center_index: int, window: int = 1) -> str:
    """
    ê°™ì€ ë¬¸ì„œ(meta_id) ì•ˆì—ì„œ center_index ì£¼ë³€ ì²­í¬ë“¤ì„ window ë²”ìœ„ë§Œí¼ í•©ì³ì„œ
    í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ë¸”ë¡ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.
    ì˜ˆ: window = 1 â†’ chunk_index 6,7,8 (Â±1 í™•ì¥)
    """
    cur.execute(
        """
        SELECT chunk_index, text
        FROM chunks
        WHERE meta_id = %s
          AND chunk_index BETWEEN %s AND %s
        ORDER BY chunk_index
        """,
        (meta_id, center_index - window, center_index + 2*window),
    )
    rows = cur.fetchall()

    # í…ìŠ¤íŠ¸ë§Œ ì •ë ¬ëœ ìˆœì„œëŒ€ë¡œ ì´ì–´ë¶™ì„
    texts = [r["text"] for r in rows if r["text"]]
    return _merge_chunks_without_overlap(texts)


#   ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ í•¨ìˆ˜ + ë¬¸ë§¥ í™•ì¥.
def semantic_search(
    query: str,
    k: int = 5,
    index_dir: str = DEFAULT_INDEX_DIR,
    window: int = 1,  # ë¬¸ë§¥ í™•ì¥ì„ ìœ„í•œ ì¶”ê°€ íŒŒë¼ë¯¸í„°
) -> List[Dict[str, Any]]:
    """
    - FAISS ì¸ë±ìŠ¤ì—ì„œ top-k chunk ê²€ìƒ‰
    - ê° chunkì˜ meta_idë¥¼ ì´ìš©í•´ metadataì—ì„œ title, url ì¡°íšŒ
    - ê°™ì€ meta_idì˜ ì£¼ë³€ ì²­í¬(chunk_index Â± window)ê¹Œì§€ ë¶™ì—¬ ë¬¸ë§¥ í™•ì¥(context_text) ìƒì„±
    - ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸[dict] í˜•íƒœë¡œ ë°˜í™˜
    """
    vectorstore, _ = load_vectorstore(index_dir=index_dir)

    # doc, score íŠœí”Œë“¤ì„ ê°€ì ¸ì˜´
    docs_and_scores = vectorstore.similarity_search_with_score(query, k=k)

    results: List[Dict[str, Any]] = []

    conn = get_connection()

    try:
        with conn.cursor() as cur:
            for doc, score in docs_and_scores:
                meta = doc.metadata
                meta_id = meta.get("meta_id")
                chunk_id = meta.get("chunk_id")
                chunk_index = meta.get("chunk_index")

                # ë¬¸ì„œ ì œëª©/URL ì¡°íšŒ
                cur.execute(
                    """
                    SELECT title, url
                    FROM metadata
                    WHERE meta_id = %s
                    """,
                    (meta_id,),
                )
                row = cur.fetchone()
                title = row["title"] if row else None
                url = row["url"] if row else None

                # ğŸ”¥ ë¬¸ë§¥ í™•ì¥ ìˆ˜í–‰
                # chunk_index ì£¼ë³€ window ë§Œí¼ ì²­í¬ë¥¼ í•©ì³ context_text ìƒì„±
                context_text = expand_context(cur, meta_id, chunk_index, window)

                # ê²°ê³¼ ì €ì¥
                results.append(
                    {
                        "meta_id": meta_id,
                        "chunk_id": chunk_id,
                        "chunk_index": chunk_index,

                        # ê²€ìƒ‰ ì ìˆ˜(ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                        "score": float(score),

                        # ë¬¸ì„œ ì •ë³´
                        "title": title,
                        "url": url,

                        # ê¸°ì¡´ ë‹¨ì¼ ì²­í¬ í…ìŠ¤íŠ¸
                        "chunk_text": doc.page_content,

                        # ğŸ”¥ ë¬¸ë§¥ í™•ì¥ëœ ë¸”ë¡ (LLMì—ëŠ” ì´ê±¸ ì£¼ë©´ ë¨)
                        "context_text": context_text,
                        "context_window": window,
                    }
                )
    finally:
        conn.close()

    return results


def semantic_search_rerank(
    query: str,
    k: int = 20,
    top_n: int = 5,
    index_dir: str = DEFAULT_INDEX_DIR,
    window: int = 1,
    rerank_model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2",
) -> List[Dict[str, Any]]:
    """
    semantic_search()ë¡œ FAISS top-k ì²­í¬ë¥¼ ê°€ì ¸ì˜¨ ë’¤ Cross-Encoderë¡œ rerankí•˜ì—¬ top_nì„ ë°˜í™˜.

    - rerankëŠ” rag_test.pyì™€ ë™ì¼í•˜ê²Œ chunk_text(ì› ì²­í¬) ê¸°ì¤€ìœ¼ë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°
    - ë°˜í™˜ dictì— rerank_score(float) ì¶”ê°€
    """
    retrieved = semantic_search(
        query=query,
        k=k,
        index_dir=index_dir,
        window=window,
    )

    if not retrieved or top_n <= 0:
        return []

    # Lazy import: sentence_transformersëŠ” í™˜ê²½ì— ë”°ë¼ ì—†ì„ ìˆ˜ ìˆìŒ.
    try:
        from sentence_transformers import CrossEncoder  # type: ignore
    except Exception as e:
        raise ImportError(
            "semantic_search_rerank requires sentence-transformers. "
            "Install it or use semantic_search() instead."
        ) from e

    reranker = CrossEncoder(rerank_model_name)
    # reranker = CrossEncoder(
    #     "Dongjin-kr/ko-reranker-base",
    #     device="cuda"
    # )

    pairs = []
    valid_rows: List[Dict[str, Any]] = []
    for r in retrieved:
        text = r.get("context_text")
        if text:
            pairs.append((query, text))
            valid_rows.append(r)

    if not pairs:
        return []

    scores = reranker.predict(pairs)

    reranked = sorted(
        zip(valid_rows, scores),
        key=lambda x: x[1],
        reverse=True,
    )

    results: List[Dict[str, Any]] = []
    for r, s in reranked[: min(top_n, len(reranked))]:
        out = dict(r)
        out["rerank_score"] = float(s)
        results.append(out)

    return results


def hybrid_search_rerank(
    query: str,
    *,
    faiss_k: int = 20,
    web_k: int = 10,
    top_n: int = 5,
    merge_k: Optional[int] = None,
    index_dir: str = DEFAULT_INDEX_DIR,
    window: int = 1,
    rerank_model_name: str = "Dongjin-kr/ko-reranker-base",
    web_search_fn: Optional[Callable[..., List[Dict[str, Any]]]] = None,
    parallel: bool = True,
) -> List[Dict[str, Any]]:
    if merge_k is None:
        merge_k = faiss_k + web_k

    def fetch_internal() -> List[Dict[str, Any]]:
        return semantic_search(query=query, k=faiss_k, index_dir=index_dir, window=window)

    def fetch_web() -> List[Dict[str, Any]]:
        if web_search_fn is None:
            try:
                from langChain_v3 import web_search  # type: ignore

                return _collect_web_results(query, web_k, web_search.search_web)
            except Exception:
                return []
        return _collect_web_results(query, web_k, web_search_fn)

    if parallel:
        with ThreadPoolExecutor(max_workers=2) as executor:
            internal_future = executor.submit(fetch_internal)
            web_future = executor.submit(fetch_web)
            internal_rows = internal_future.result()
            web_rows = web_future.result()
    else:
        internal_rows = fetch_internal()
        web_rows = fetch_web()

    internal_rows = _attach_source(internal_rows, "internal")
    web_rows = _normalize_web_results(web_rows)

    merged = _dedupe_results(list(internal_rows) + list(web_rows))
    merged = merged[:merge_k]

    if not merged or top_n <= 0:
        return []

    try:
        from sentence_transformers import CrossEncoder  # type: ignore
    except Exception as e:
        raise ImportError(
            "hybrid_search_rerank requires sentence-transformers."
        ) from e

    reranker = CrossEncoder(rerank_model_name, device=_default_rerank_device())

    pairs: List[tuple[str, str]] = []
    valid_rows: List[Dict[str, Any]] = []
    for row in merged:
        # text = row.get("context_text") or row.get("chunk_text") or row.get("snippet")
        text = row.get("chunk_text") or row.get("snippet")
        if text:
            pairs.append((query, text))
            valid_rows.append(row)

    if not pairs:
        return []

    scores = reranker.predict(pairs)
    reranked = sorted(
        zip(valid_rows, scores),
        key=lambda x: x[1],
        reverse=True,
    )

    results: List[Dict[str, Any]] = []
    for row, score in reranked[: min(top_n, len(reranked))]:
        out = dict(row)
        out["rerank_score"] = float(score)
        results.append(out)

    return results
