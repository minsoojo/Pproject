# # í¬ë¡¤ë§ ìˆ˜í–‰ ì½”ë“œ

# #!/usr/bin/env python
# # -*- coding: utf-8 -*-

# import os
# import time
# import json
# from urllib.parse import urljoin, urlparse, urldefrag
# from bs4 import BeautifulSoup

# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.chrome.service import Service
# from selenium.common.exceptions import UnexpectedAlertPresentException
# from webdriver_manager.chrome import ChromeDriverManager


# # ============================================================
# # ì„¤ì •
# # ============================================================
# START_URL = "https://www.gachon.ac.kr/kor/3120/subview.do"
# DOMAIN = "www.gachon.ac.kr"

# DATA_DIR = "data"
# HTML_DIR = os.path.join(DATA_DIR, "html")
# TEXT_DIR = os.path.join(DATA_DIR, "text")
# FILE_DIR = os.path.join(DATA_DIR, "files")
# FILE_TEXT_DIR = os.path.join(DATA_DIR, "file_text")

# os.makedirs(HTML_DIR, exist_ok=True)
# os.makedirs(TEXT_DIR, exist_ok=True)
# os.makedirs(FILE_DIR, exist_ok=True)
# os.makedirs(FILE_TEXT_DIR, exist_ok=True)

# VISITED_FILE = "data/visited.txt"
# METADATA_FILE = "data/metadata.jsonl"
# QUEUE_FILE = "data/queue.txt"


# # ============================================================
# # íŒŒì¼ í™•ì¥ì + ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´
# # ============================================================
# FILE_EXTS = [
#     ".pdf", ".hwp", ".hwpx",
#     ".doc", ".docx",
#     ".xls", ".xlsx",
#     ".ppt", ".pptx",
#     ".zip"
# ]

# DOWNLOAD_URL_PATTERNS = [
#     "download.do",
# ]

# # ì¶”ê°€ ì°¨ë‹¨ íŒ¨í„´: synap ë·°ì–´ + ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
# BLOCK_PATTERNS = [
#     # "synap",
#     "synapview.do",
#     # "synapviewer",

#     "/eng/",
#     "/english/",
#     "/chi/",
#     "/chn/",
#     "/china/",
# ]


# def is_file(url: str) -> bool:
#     u = url.lower()

#     if any(u.endswith(ext) for ext in FILE_EXTS):
#         return True

#     if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
#         return True

#     return False


# def is_download_url(url: str) -> bool:
#     u = url.lower()
#     return any(pat in u for pat in DOWNLOAD_URL_PATTERNS)


# def is_blocked_url(url: str) -> bool:
#     """HTML ë°©ë¬¸ì—ì„œ ì œì™¸í•´ì•¼ í•  URL"""
#     u = url.lower()

#     # 1) download.do â†’ HTML ë°©ë¬¸ ê¸ˆì§€
#     if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
#         return True

#     # 2) synap ë¬¸ì„œë·°ì–´ í˜ì´ì§€
#     if any(pat in u for pat in ["synap", "synapview", "synapviewer"]):
#         return True

#     # 3) ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
#     if any(pat in u for pat in ["/eng/", "/english/", "/chi/", "/chn/", "/china/"]):
#         return True

#     return False


# # ============================================================
# # ë“œë¼ì´ë²„ ìƒì„±
# # ============================================================
# def create_driver():
#     chrome_options = Options()

#     # headless ê¸ˆì§€
#     chrome_options.add_argument("--headless=new")

#     chrome_options.add_argument("--disable-gpu")
#     chrome_options.add_argument("--no-sandbox")
#     chrome_options.add_argument("--disable-dev-shm-usage")
#     chrome_options.add_argument("--disable-notifications")
#     chrome_options.add_argument("--disable-popup-blocking")

#     chrome_options.binary_location = "/home/t25315/chromium-portable/chrome-linux/chrome"
    

#     chrome_options.add_argument(
#         "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
#         "AppleWebKit/537.36 (KHTML, like Gecko) "
#         "Chrome/120.0.0.0 Safari/537.36"
#     )

#     # PDF ë·°ì–´ ë„ê³  ìë™ ë‹¤ìš´ë¡œë“œ
#     prefs = {
#         "download.default_directory": os.path.abspath(FILE_DIR),
#         "download.prompt_for_download": False,
#         "download.directory_upgrade": True,
#         "plugins.always_open_pdf_externally": True,
#     }
#     chrome_options.add_experimental_option("prefs", prefs)

#     # service = Service(ChromeDriverManager().install())
#     service = Service("/home/t25315/chromium-portable/chromedriver_linux64/chromedriver")
#     driver = webdriver.Chrome(service=service, options=chrome_options)

#     driver.set_page_load_timeout(14)
#     driver.set_script_timeout(14)

#     return driver


# # ============================================================
# # ìœ í‹¸ í•¨ìˆ˜
# # ============================================================
# # def canonicalize(url):
# #     url, _ = urldefrag(url)
# #     return url
# import re
# from urllib.parse import urlparse, urlunparse, urldefrag
# def canonicalize(url: str) -> str:
#     # 1) fragment ì œê±° (# ë’¤ ì œê±°)
#     url, _ = urldefrag(url)

#     # 2) ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì •ë¦¬ (:// ì œì™¸)
#     url = re.sub(r'(?<!:)//+', '/', url)

#     # 3) '..' ê°™ì€ ì´ìƒí•œ dot íŒ¨í„´ ì •ë¦¬
#     url = url.replace("..do", ".do")
#     url = url.replace("..html", ".html")
#     url = url.replace("..php", ".php")
    
#     # 4) ë§ˆì§€ë§‰ì— '/' ë¶™ì§€ ì•Šê²Œ
#     if url.endswith("/"):
#         url = url[:-1]

#     # 5) URL êµ¬ì¡° ë‹¤ì‹œ ì¡°í•©
#     parsed = urlparse(url)
#     normalized = urlunparse(parsed)

#     return normalized


# def load_visited():
#     if os.path.exists(VISITED_FILE):
#         with open(VISITED_FILE) as f:
#             return set(line.strip() for line in f)
#     return set()


# def save_visited(url):
#     with open(VISITED_FILE, "a") as f:
#         f.write(url + "\n")


# def load_queue():
#     if os.path.exists(QUEUE_FILE):
#         with open(QUEUE_FILE) as f:
#             return [line.strip() for line in f if line.strip()]
#     return []


# def save_queue(queue):
#     with open(QUEUE_FILE, "w") as f:
#         f.write("\n".join(queue))


# # def html_to_text(html):
# #     soup = BeautifulSoup(html, "html.parser")
# #     for tag in soup(["script", "style", "noscript", "footer", "nav"]):
# #         tag.decompose()

# #     text = soup.get_text(separator="\n")
# #     return "\n".join(x.strip() for x in text.splitlines() if x.strip())


# def html_to_text(html: str) -> str:
#     # soup = BeautifulSoup(html, "html.parser")
#     soup = BeautifulSoup(html, "lxml")

#     # 1) í™•ì‹¤íˆ í•„ìš” ì—†ëŠ” íƒœê·¸ ì œê±°
#     for tag in soup(["script", "style", "noscript"]):
#         tag.decompose()

#     # 2) header/footer íƒœê·¸ ì§ì ‘ ì œê±°
#     for tag in soup.find_all(["header", "footer"]):
#         tag.decompose()

#     # 3) ì‚¬ì´íŠ¸ì—ì„œ ìì£¼ ì“°ëŠ” í—¤ë”/í‘¸í„° CSS ID/class ì œê±°
#     header_footer_selectors = [
#         "#header",          # ê°€ì²œëŒ€ ìµœìƒë‹¨ í—¤ë”
#         "#footer",          # í˜ì´ì§€ í‘¸í„°
#         ".header",          # ê³µí†µ header class
#         ".footer",          # ê³µí†µ footer class
#         ".gnb",             # ê¸€ë¡œë²Œ ë„¤ë¹„ê²Œì´ì…˜ ë°”
#         "#gnb",             # ë©”ë‰´
#         ".logo",            # ìƒë‹¨ ë¡œê³  ì˜ì—­
#         ".site-map",        # ì‚¬ì´íŠ¸ë§µ
#         ".sub-visual",      # ì„œë¸Œ ë¹„ì£¼ì–¼ ì´ë¯¸ì§€
#         ".top-banner",      # ìƒë‹¨ ë°°ë„ˆ
#         ".bottom-banner",   # í•˜ë‹¨ ë°°ë„ˆ
#         ".quick-menu",      # í€µë©”ë‰´
#         ".breadcrumb",      # í˜„ì¬ ìœ„ì¹˜ breadcrumb
#         ".location",        # ìœ„ì¹˜ í‘œì‹œ UI
#         ".nav",             # ë„¤ë¹„ê²Œì´ì…˜
#         ".menu",            # ë©”ë‰´ ì „ì²´
#         ".wrap-header",
#         ".wrap-footer",
#         "#wrap-header",
#         "#wrap-footer",
#         ".sns-area",
#         ".top-menu",
#     ]

#     # for selector in header_footer_selectors:
#     #     for tag in soup.select(selector):
#     #         tag.decompose()
#     for selector in header_footer_selectors:
#         try:
#             for tag in soup.select(selector):
#                 tag.decompose()
#         except Exception:
#             continue

#     # 4) ê°€ì²œëŒ€ ë³¸ë¬¸ ì˜ì—­(#content, #contents, .content-wrapper ë“±)
#     main_candidates = [
#         "#content",
#         "#contents",
#         ".content",
#         ".contents",
#         ".content-area",
#         ".content-wrapper",
#         ".sub-content",
#         ".sub-contents",
#         ".article",
#         ".board-view",     # ê²Œì‹œíŒ ë³¸ë¬¸
#         "#container",
#     ]

#     main = None

#     for sel in main_candidates:
#         area = soup.select_one(sel)
#         if area:
#             main = area
#             break   
        
#     if main is None:
#         candidates = soup.find_all(["article", "section"])
#         if candidates:
#             main = max(
#                 candidates,
#                 key=lambda c: len(c.get_text(strip=True))
#             )

#     if main is None:  
#         main = soup  # fallback

#     markdown_tables = []

#     tables = main.find_all("table")
#     for t in tables:
#         rows = t.find_all("tr")
#         table_lines = []
#         for i, row in enumerate(rows):
#             cells = row.find_all(["th", "td"])
#             cell_texts = [c.get_text(strip=True) for c in cells]
#             # ë¹ˆ í–‰ì€ ìŠ¤í‚µ
#             if not any(cell_texts):
#                 continue

#             # ë³¸ë¬¸
#             table_lines.append("| " + " | ".join(cell_texts) + " |")

#             # ì²« í–‰ì„ í—¤ë”ë¡œ ë³´ê³  êµ¬ë¶„ì„  ì¶”ê°€
#             if i == 0:
#                 table_lines.append("| " + " | ".join(["---"] * len(cell_texts)) + " |")

#         if table_lines:
#             markdown_tables.append("\n".join(table_lines))

#     # 5-2) table ì„ DOMì—ì„œ ì œê±°í•´ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì¤‘ë³µ ë°©ì§€
#     for t in tables:
#         t.decompose()

#     # 6) ë³¸ë¬¸ í…ìŠ¤íŠ¸
#     text = main.get_text(separator="\n")
#     lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
#     body_text = "\n".join(lines)

#     # 7) í…Œì´ë¸”ì„ markdownìœ¼ë¡œ ë¶™ì—¬ì¤Œ
#     if markdown_tables:
#         return body_text + "\n\n" + "\n\n".join(markdown_tables)
#     return body_text    

# def save_text(content, idx):
#     fpath = os.path.join(TEXT_DIR, f"{idx:05d}.txt")
#     with open(fpath, "w", encoding="utf-8") as f:
#         f.write(content)
#     return fpath


# # ============================================================
# # ì•ˆì „ GET
# # ============================================================
# def safe_get(driver, url):
#     try:
#         driver.get(url)
#         time.sleep(0.1)

#         try:
#             alert = driver.switch_to.alert
#             print("[ALERT FOUND]", alert.text)
#             alert.accept()
#             return False
#         except:
#             pass

#         return True

#     except Exception as e:
#         print(f"[ERROR safe_get] {e}")
#         return False
    
# # ============================================================
# # wait_for_download
# # ============================================================

# def wait_for_download(dir_path, timeout=15):
#     """
#     ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ë¥¼ ê°ì‹œí•˜ì—¬ ìƒˆ íŒŒì¼ì´ ìƒì„±ë˜ë©´ í•´ë‹¹ ê²½ë¡œë¥¼ ë°˜í™˜
#     """
#     before = set(os.listdir(dir_path))
#     for _ in range(timeout * 10):  # 0.1ì´ˆ ê°„ê²©
#         time.sleep(0.1)
#         after = set(os.listdir(dir_path))
#         new_files = after - before
#         if new_files:
#             return os.path.join(dir_path, list(new_files)[0])
#     return None
# # ============================================================
# # ë©”ì¸ í¬ë¡¤ëŸ¬
# # ============================================================
# def crawl():
#     driver = create_driver()

#     visited = load_visited()
#     queue = load_queue()
    
#     start = canonicalize(START_URL)
#     if not queue:
#         queue.append(start)

#     seen = set(visited) | set(queue)
#     idx = len(visited)

#     print("[INFO] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
#     print(f"[INFO] ë°©ë¬¸ ì™„ë£Œ {len(visited)}ê°œ")
#     print(f"[INFO] ëŒ€ê¸° ì¤‘ {len(queue)}ê°œ\n")

#     while queue:
#         url = queue.pop(0)
#         idx += 1

#         print(f"\n[{idx}] GET {url}")
        
#         # ğŸ”¥ HTML ì°¨ë‹¨ íŒ¨í„´
#         if is_blocked_url(url):
#             print("    [SKIP] ì°¨ë‹¨ëœ URL:", url)
#             visited.add(url)
#             save_visited(url)
#             save_queue(queue)
#             continue

#         # ğŸ”¥ download.do ìì²´ëŠ” HTML ë°©ë¬¸ ê¸ˆì§€
#         if is_download_url(url):
#             print("    [SKIP] download URL:", url)
#             visited.add(url)
#             save_visited(url)
#             save_queue(queue)
#             continue

#         # ë°©ë¬¸
#         success = safe_get(driver, url)
#         if not success:
#             print("[SKIP] alert/ì˜¤ë¥˜:", url)
#             visited.add(url)
#             save_visited(url)
#             save_queue(queue)
#             continue

#         html = driver.page_source

#         # HTML â†’ TEXT ì €ì¥
#         html_text = html_to_text(html)
#         html_text_path = save_text(html_text, idx)

#         # metadata ì €ì¥
#         soup = BeautifulSoup(html, "lxml")
#         title = soup.title.string if soup.title else ""

#         metadata = {
#             "id": idx,
#             "type": "html",
#             "url": url,
#             "text": html_text_path,
#             "title": title,
#             "timestamp": time.time()
#         }
#         with open(METADATA_FILE, "a", encoding="utf-8") as f:
#             f.write(json.dumps(metadata, ensure_ascii=False) + "\n")    

#         # -------------------------
#         # ë§í¬ íƒìƒ‰
#         # -------------------------
#         for a in soup.find_all("a", href=True):
#             href = a["href"].strip()

#             if href.startswith("javascript:") or href.startswith("mailto:"):
#                 continue

#             next_url = canonicalize(urljoin(url, href))

#             if next_url.startswith("http://"):
#                 continue

#             # ğŸ”¥ íŒŒì¼ ì²˜ë¦¬
#             if is_file(next_url):
#                 print(f"    [FILE LINK] {next_url}")

#                 ext = os.path.splitext(next_url)[1].lower()
#                 if ext == "":
#                     ext = ".temp"

#                 try:
#                     driver.execute_script("window.open(arguments[0]);", next_url)
#                 except:
#                     pass
                
#                 downloaded_path = wait_for_download(FILE_DIR)

#                 file_meta = {
#                     "id": f"file-{idx}-{len(os.listdir(FILE_DIR))+1}",
#                     "type": f"file:{ext}",
#                     "url": next_url,
#                     "ref_page_url": url,
#                     "ref_page_id": idx,
#                     "file_path": downloaded_path,
#                     "timestamp": time.time()
#                 }
#                 with open(METADATA_FILE, "a", encoding="utf-8") as f:
#                     f.write(json.dumps(file_meta, ensure_ascii=False) + "\n")
#                 continue

#             # ğŸ”¥ HTML ì°¨ë‹¨ URLì€ ë°©ë¬¸ ê¸ˆì§€
#             if is_blocked_url(next_url):
#                 continue

#             # ğŸ”¥ ì™¸ë¶€ ë„ë©”ì¸ ê¸ˆì§€
#             if urlparse(next_url).netloc != DOMAIN:
#                 continue

#             if next_url not in seen:
#                 seen.add(next_url)
#                 queue.append(next_url)
#                 save_queue(queue)

#         visited.add(url)
#         save_visited(url)
#         save_queue(queue)

#     driver.quit()
#     print("\n[DONE] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ")


# # ì‹¤í–‰
# if __name__ == "__main__":
#     crawl()

# í¬ë¡¤ë§ ìˆ˜í–‰ ì½”ë“œ

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import time
import json
from urllib.parse import urljoin, urlparse, urldefrag
from bs4 import BeautifulSoup
from collections import defaultdict


from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import UnexpectedAlertPresentException
from webdriver_manager.chrome import ChromeDriverManager


# ============================================================
# ì„¤ì •
# ============================================================
START_URL = "https://www.gachon.ac.kr/kor/3120/subview.do"
DOMAIN = "www.gachon.ac.kr"

DATA_DIR = "data"
HTML_DIR = os.path.join(DATA_DIR, "html")
TEXT_DIR = os.path.join(DATA_DIR, "text")
FILE_DIR = os.path.join(DATA_DIR, "files")
FILE_TEXT_DIR = os.path.join(DATA_DIR, "file_text")

os.makedirs(HTML_DIR, exist_ok=True)
os.makedirs(TEXT_DIR, exist_ok=True)
os.makedirs(FILE_DIR, exist_ok=True)
os.makedirs(FILE_TEXT_DIR, exist_ok=True)

VISITED_FILE = "data/visited.txt"
METADATA_FILE = "data/metadata.jsonl"
QUEUE_FILE = "data/queue.txt"


# ============================================================
# íƒ€ê²Ÿ HTML URL íŒë³„ (bbs, artclView.do ë“±)
# ============================================================
TARGET_KEYWORDS = [
    "bbs",
    "artclview.do",   # ì‹¤ì œ ì†ŒìŠ¤ëŠ” ëŒ€ì†Œë¬¸ì ì„ì—¬ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì†Œë¬¸ìë¡œ í†µì¼
    "artclVIew.do",
    "subview.do",
]

def is_target_html_url(url: str) -> bool:
    u = url.lower()
    return any(kw.lower() in u for kw in TARGET_KEYWORDS)


# ============================================================
# íŒŒì¼ í™•ì¥ì + ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´
# ============================================================
FILE_EXTS = [
    ".pdf", ".hwp", ".hwpx",
    ".doc", ".docx",
    ".xls", ".xlsx",
    ".ppt", ".pptx",
    ".zip"
]

DOWNLOAD_URL_PATTERNS = [
    "download.do",
]

# ì¶”ê°€ ì°¨ë‹¨ íŒ¨í„´: synap ë·°ì–´ + ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
BLOCK_PATTERNS = [
    # "synap",
    "synapview.do",
    # "synapviewer",

    "/eng/",
    "/english/",
    "/chi/",
    "/chn/",
    "/china/",
]


def is_file(url: str) -> bool:
    u = url.lower()

    if any(u.endswith(ext) for ext in FILE_EXTS):
        return True

    if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
        return True

    return False


def is_download_url(url: str) -> bool:
    u = url.lower()
    return any(pat in u for pat in DOWNLOAD_URL_PATTERNS)


def is_blocked_url(url: str) -> bool:
    """HTML ë°©ë¬¸ì—ì„œ ì œì™¸í•´ì•¼ í•  URL"""
    u = url.lower()

    # 1) download.do â†’ HTML ë°©ë¬¸ ê¸ˆì§€
    if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
        return True

    # 2) synap ë¬¸ì„œë·°ì–´ í˜ì´ì§€
    if any(pat in u for pat in ["synap", "synapview", "synapviewer"]):
        return True

    # 3) ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
    if any(pat in u for pat in ["/eng/", "/english/", "/chi/", "/chn/", "/china/"]):
        return True

    return False


# ============================================================
# ë“œë¼ì´ë²„ ìƒì„±
# ============================================================
def create_driver():
    chrome_options = Options()

    # headless ê¸ˆì§€
    chrome_options.add_argument("--headless=new")

    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--disable-popup-blocking")

    chrome_options.binary_location = "/home/t25315/chromium-portable/chrome-linux/chrome"
    

    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )

    # PDF ë·°ì–´ ë„ê³  ìë™ ë‹¤ìš´ë¡œë“œ
    prefs = {
        "download.default_directory": os.path.abspath(FILE_DIR),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "plugins.always_open_pdf_externally": True,
    }
    chrome_options.add_experimental_option("prefs", prefs)

    # service = Service(ChromeDriverManager().install())
    service = Service("/home/t25315/chromium-portable/chromedriver_linux64/chromedriver")
    driver = webdriver.Chrome(service=service, options=chrome_options)

    driver.set_page_load_timeout(60)
    driver.set_script_timeout(60)

    return driver


# ============================================================
# ìœ í‹¸ í•¨ìˆ˜
# ============================================================
# def canonicalize(url):
#     url, _ = urldefrag(url)
#     return url
import re
from urllib.parse import urlparse, urlunparse, urldefrag
def canonicalize(url: str) -> str:
    # 1) fragment ì œê±° (# ë’¤ ì œê±°)
    url, _ = urldefrag(url)

    # 2) ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì •ë¦¬ (:// ì œì™¸)
    url = re.sub(r'(?<!:)//+', '/', url)

    # 3) '..' ê°™ì€ ì´ìƒí•œ dot íŒ¨í„´ ì •ë¦¬
    url = url.replace("..do", ".do")
    url = url.replace("..html", ".html")
    url = url.replace("..php", ".php")
    
    # 4) ë§ˆì§€ë§‰ì— '/' ë¶™ì§€ ì•Šê²Œ
    if url.endswith("/"):
        url = url[:-1]

    # 5) URL êµ¬ì¡° ë‹¤ì‹œ ì¡°í•©
    parsed = urlparse(url)
    normalized = urlunparse(parsed)

    return normalized

def extract_pagination_urls(current_url: str, href: str):
    """
    javascript:ë¡œ ëœ í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ì—ì„œ ì‹¤ì œ í˜ì´ì§€ URLì„ ë§Œë“¤ì–´ë‚´ëŠ” í•¨ìˆ˜.

    ì˜ˆì‹œ íŒ¨í„´ (ê°€ì •):
      - javascript:fnList('3120','2')
      - javascript:goPage(2)

    ì‹¤ì œë¡œ ê°€ì²œëŒ€ ì†ŒìŠ¤ì—ì„œ í˜ì´ì§€ë„¤ì´ì…˜ hrefë¥¼ ë³´ê³ 
    ì•„ë˜ ì •ê·œì‹/URL ìƒì„± ë¶€ë¶„ì„ ë§ì¶°ì„œ ì¡°ì •í•˜ë©´ ë¨.
    """
    urls = []

    # íŒ¨í„´ 1) fnList('ê²Œì‹œíŒID','í˜ì´ì§€ë²ˆí˜¸')
    m = re.search(r"fnList\('(\d+)'\s*,\s*'(\d+)'\)", href)
    if m:
        board_id = m.group(1)   # 3120
        page_no  = m.group(2)   # 2
        base = current_url.split("?", 1)[0]

        # âš ï¸ ê°€ì²œëŒ€ ì‹¤ì œ íŒŒë¼ë¯¸í„° ì´ë¦„ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        # ì˜ˆ: pageIndex, page, pageNo ë“±
        urls.append(f"{base}?pageIndex={page_no}")
        return urls

    # íŒ¨í„´ 2) goPage(2) ê°™ì€ í˜•íƒœ
    m = re.search(r"goPage\((\d+)\)", href)
    if m:
        page_no = m.group(1)
        base = current_url.split("?", 1)[0]
        urls.append(f"{base}?pageIndex={page_no}")
        return urls

    return urls


def load_visited():
    if os.path.exists(VISITED_FILE):
        with open(VISITED_FILE) as f:
            return set(line.strip() for line in f)
    return set()


def save_visited(url):
    with open(VISITED_FILE, "a") as f:
        f.write(url + "\n")


def load_queue():
    if os.path.exists(QUEUE_FILE):
        with open(QUEUE_FILE) as f:
            return [line.strip() for line in f if line.strip()]
    return []


def save_queue(queue):
    with open(QUEUE_FILE, "w") as f:
        f.write("\n".join(queue))


# def html_to_text(html):
#     soup = BeautifulSoup(html, "html.parser")
#     for tag in soup(["script", "style", "noscript", "footer", "nav"]):
#         tag.decompose()

#     text = soup.get_text(separator="\n")
#     return "\n".join(x.strip() for x in text.splitlines() if x.strip())


def html_to_text(html: str) -> str:
    # soup = BeautifulSoup(html, "html.parser")
    soup = BeautifulSoup(html, "lxml")

    # 1) í™•ì‹¤íˆ í•„ìš” ì—†ëŠ” íƒœê·¸ ì œê±°
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # 2) header/footer íƒœê·¸ ì§ì ‘ ì œê±°
    for tag in soup.find_all(["header", "footer"]):
        tag.decompose()

    # 3) ì‚¬ì´íŠ¸ì—ì„œ ìì£¼ ì“°ëŠ” í—¤ë”/í‘¸í„° CSS ID/class ì œê±°
    header_footer_selectors = [
        "#header",          # ê°€ì²œëŒ€ ìµœìƒë‹¨ í—¤ë”
        "#footer",          # í˜ì´ì§€ í‘¸í„°
        ".header",          # ê³µí†µ header class
        ".footer",          # ê³µí†µ footer class
        ".gnb",             # ê¸€ë¡œë²Œ ë„¤ë¹„ê²Œì´ì…˜ ë°”
        "#gnb",             # ë©”ë‰´
        ".logo",            # ìƒë‹¨ ë¡œê³  ì˜ì—­
        ".site-map",        # ì‚¬ì´íŠ¸ë§µ
        ".sub-visual",      # ì„œë¸Œ ë¹„ì£¼ì–¼ ì´ë¯¸ì§€
        ".top-banner",      # ìƒë‹¨ ë°°ë„ˆ
        ".bottom-banner",   # í•˜ë‹¨ ë°°ë„ˆ
        ".quick-menu",      # í€µë©”ë‰´
        ".breadcrumb",      # í˜„ì¬ ìœ„ì¹˜ breadcrumb
        ".location",        # ìœ„ì¹˜ í‘œì‹œ UI
        ".nav",             # ë„¤ë¹„ê²Œì´ì…˜
        ".menu",            # ë©”ë‰´ ì „ì²´
        ".wrap-header",
        ".wrap-footer",
        "#wrap-header",
        "#wrap-footer",
        ".sns-area",
        ".top-menu",
    ]

    # for selector in header_footer_selectors:
    #     for tag in soup.select(selector):
    #         tag.decompose()
    for selector in header_footer_selectors:
        try:
            for tag in soup.select(selector):
                tag.decompose()
        except Exception:
            continue

    # 4) ê°€ì²œëŒ€ ë³¸ë¬¸ ì˜ì—­(#content, #contents, .content-wrapper ë“±)
    main_candidates = [
        "#content",
        "#contents",
        ".content",
        ".contents",
        ".content-area",
        ".content-wrapper",
        ".sub-content",
        ".sub-contents",
        ".article",
        ".board-view",     # ê²Œì‹œíŒ ë³¸ë¬¸
        "#container",
    ]

    main = None

    for sel in main_candidates:
        area = soup.select_one(sel)
        if area:
            main = area
            break   
        
    if main is None:
        candidates = soup.find_all(["article", "section"])
        if candidates:
            main = max(
                candidates,
                key=lambda c: len(c.get_text(strip=True))
            )

    if main is None:  
        main = soup  # fallback

    markdown_tables = []

    tables = main.find_all("table")
    for t in tables:
        rows = t.find_all("tr")
        table_lines = []
        for i, row in enumerate(rows):
            cells = row.find_all(["th", "td"])
            cell_texts = [c.get_text(strip=True) for c in cells]
            # ë¹ˆ í–‰ì€ ìŠ¤í‚µ
            if not any(cell_texts):
                continue

            # ë³¸ë¬¸
            table_lines.append("| " + " | ".join(cell_texts) + " |")

            # ì²« í–‰ì„ í—¤ë”ë¡œ ë³´ê³  êµ¬ë¶„ì„  ì¶”ê°€
            if i == 0:
                table_lines.append("| " + " | ".join(["---"] * len(cell_texts)) + " |")

        if table_lines:
            markdown_tables.append("\n".join(table_lines))

    # 5-2) table ì„ DOMì—ì„œ ì œê±°í•´ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì¤‘ë³µ ë°©ì§€
    for t in tables:
        t.decompose()

    # 6) ë³¸ë¬¸ í…ìŠ¤íŠ¸
    text = main.get_text(separator="\n")
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    body_text = "\n".join(lines)

    # 7) í…Œì´ë¸”ì„ markdownìœ¼ë¡œ ë¶™ì—¬ì¤Œ
    if markdown_tables:
        return body_text + "\n\n" + "\n\n".join(markdown_tables)
    return body_text    

def save_text(content, idx):
    fpath = os.path.join(TEXT_DIR, f"{idx:05d}.txt")
    with open(fpath, "w", encoding="utf-8") as f:
        f.write(content)
    return fpath


# ============================================================
# ì•ˆì „ GET
# ============================================================
def safe_get(driver, url):
    try:
        driver.get(url)
        time.sleep(0.1)

        try:
            alert = driver.switch_to.alert
            print("[ALERT FOUND]", alert.text)
            alert.accept()
            return False
        except:
            pass

        return True

    except Exception as e:
        print(f"[ERROR safe_get] {e}")
        return False
    
# ============================================================
# wait_for_download
# ============================================================

def wait_for_download(dir_path, timeout=15):
    """
    ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ë¥¼ ê°ì‹œí•˜ì—¬ ìƒˆ íŒŒì¼ì´ ìƒì„±ë˜ë©´ í•´ë‹¹ ê²½ë¡œë¥¼ ë°˜í™˜
    """
    before = set(os.listdir(dir_path))
    for _ in range(timeout * 10):  # 0.1ì´ˆ ê°„ê²©
        time.sleep(0.1)
        after = set(os.listdir(dir_path))
        new_files = after - before
        if new_files:
            return os.path.join(dir_path, list(new_files)[0])
    return None

# ============================================================
# ë©”ì¸ í¬ë¡¤ëŸ¬
# ============================================================
def crawl():
    driver = create_driver()
    
    for path in [VISITED_FILE, QUEUE_FILE]:
        if os.path.exists(path):
            os.remove(path)

    # visited = load_visited()
    # queue = load_queue()
    
    start = canonicalize(START_URL)

    visited = set()
    queue = [start]

    seen = set(queue)
    idx = len(visited)

    # if not queue:
    #     queue.append(start)

    # seen = set(visited) | set(queue)
    # idx = len(visited)

    pagination_stats = defaultdict(set)

    print("[INFO] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
    print(f"[INFO] ë°©ë¬¸ ì™„ë£Œ {len(visited)}ê°œ")
    print(f"[INFO] ëŒ€ê¸° ì¤‘ {len(queue)}ê°œ\n")

    while queue:
        url = queue.pop(0)
        idx += 1

        print(f"\n[{idx}] GET {url}")
        
        # ğŸ”¥ HTML ì°¨ë‹¨ íŒ¨í„´
        if is_blocked_url(url):
            print("    [SKIP] ì°¨ë‹¨ëœ URL:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        # ğŸ”¥ download.do ìì²´ëŠ” HTML ë°©ë¬¸ ê¸ˆì§€
        if is_download_url(url):
            print("    [SKIP] download URL:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        # ë°©ë¬¸
        success = safe_get(driver, url)
        if not success:
            print("[SKIP] alert/ì˜¤ë¥˜ (ì¬ì‹œë„ í›„ë³´):", url)
            # â— ì‹¤íŒ¨í•œ URLì€ visitedì— ë„£ì§€ ì•ŠëŠ”ë‹¤ (ë‚˜ì¤‘ì— ë‹¤ì‹œ ëŒë¦´ ìˆ˜ ìˆê²Œ)
            save_queue(queue)
            continue

        html = driver.page_source

        # HTML â†’ TEXT ì €ì¥
        html_text = html_to_text(html)
        html_text_path = save_text(html_text, idx)

        # metadata ì €ì¥
        soup = BeautifulSoup(html, "lxml")
        title = soup.title.string if soup.title else ""

        metadata = {
            "id": idx,
            "type": "html",
            "url": url,
            "text": html_text_path,
            "title": title,
            "timestamp": time.time()
        }
        with open(METADATA_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(metadata, ensure_ascii=False) + "\n")    

        # -------------------------
        # ë§í¬ íƒìƒ‰
        # -------------------------
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()

            # 1) javascript: ë§í¬ â†’ í˜ì´ì§€ë„¤ì´ì…˜ë§Œ íŠ¹ë³„ ì²˜ë¦¬
            if href.startswith("javascript:"):
                paginated_urls = extract_pagination_urls(url, href)
                if paginated_urls:
                    for p in paginated_urls:
                        next_url = canonicalize(urljoin(url, p))

                        # ì™¸ë¶€ ë„ë©”ì¸ ë°©ì§€
                        if urlparse(next_url).netloc != DOMAIN:
                            continue

                        if next_url not in seen:
                            seen.add(next_url)
                            queue.append(next_url)
                            save_queue(queue)

                            # í†µê³„ìš©: base_url + pageIndex ê¸°ë¡
                            base = next_url.split("?", 1)[0]
                            m_page = re.search(r"pageIndex=(\d+)", next_url)
                            if m_page:
                                page_no = int(m_page.group(1))
                                pagination_stats[base].add(page_no)

                            print(f"    [PAGINATION] {href} -> {next_url}")

                # í˜ì´ì§€ë„¤ì´ì…˜ì´ ì•„ë‹ˆê±°ë‚˜, ì²˜ë¦¬ ì‹¤íŒ¨í•œ javascript:ëŠ” ê·¸ëƒ¥ ìŠ¤í‚µ
                continue

            # 2) mailto:ëŠ” ê·¸ëŒ€ë¡œ ìŠ¤í‚µ
            if href.startswith("mailto:"):
                continue

            # 3) ë‚˜ë¨¸ì§€ ì¼ë°˜ hrefëŠ” ê¸°ì¡´ê³¼ ë™ì¼
            next_url = canonicalize(urljoin(url, href))

            if next_url.startswith("http://"):
                continue

            if is_file(next_url):
                print(f"    [FILE LINK] {next_url}")

                ext = os.path.splitext(next_url)[1].lower()
                if ext == "":
                    ext = ".temp"

                try:
                    driver.execute_script("window.open(arguments[0]);", next_url)
                except:
                    pass
                
                downloaded_path = wait_for_download(FILE_DIR)

                file_meta = {
                    "id": f"file-{idx}-{len(os.listdir(FILE_DIR))+1}",
                    "type": f"file:{ext}",
                    "url": next_url,
                    "ref_page_url": url,
                    "ref_page_id": idx,
                    "file_path": downloaded_path,
                    "timestamp": time.time()
                }
                with open(METADATA_FILE, "a", encoding="utf-8") as f:
                    f.write(json.dumps(file_meta, ensure_ascii=False) + "\n")
                continue


            # ğŸ”¥ HTML ì°¨ë‹¨ URLì€ ë°©ë¬¸ ê¸ˆì§€
            if is_blocked_url(next_url):
                continue

            # ğŸ”¥ ì™¸ë¶€ ë„ë©”ì¸ ê¸ˆì§€
            if urlparse(next_url).netloc != DOMAIN:
                continue

            if not is_target_html_url(next_url) and next_url != START_URL:
                # ê²Œì‹œê¸€/ê²Œì‹œíŒ URLì´ ì•„ë‹ˆë©´ BFS ëŒ€ìƒì—ì„œ ì œì™¸
                # (íŒŒì¼ ë§í¬ëŠ” ìœ„ì—ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ë„ë‹¬ X)
                continue

            if next_url not in seen:
                seen.add(next_url)
                queue.append(next_url)
                save_queue(queue)


        visited.add(url)
        save_visited(url)
        save_queue(queue)

    
    driver.quit()

    # ğŸ“Š í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì•½
    print("\n[PAGINATION STATS]")
    if not pagination_stats:
        print("  (no pagination info collected)")
    else:
        for base, pages in pagination_stats.items():
            if not pages:
                continue
            pages_sorted = sorted(pages)
            print(f"  {base}: visited pages {pages_sorted} (total {len(pages_sorted)})")

    print("\n[DONE] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ")



# ì‹¤í–‰
if __name__ == "__main__":
    crawl()