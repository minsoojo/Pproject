# #!/usr/bin/env python
# # -*- coding: utf-8 -*-

# import os
# import time
# import json
# import sqlite3
# from datetime import datetime
# from pathlib import Path
# from urllib.parse import urljoin, urlparse, urldefrag

# from bs4 import BeautifulSoup

# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.chrome.service import Service
# from selenium.common.exceptions import UnexpectedAlertPresentException
# from webdriver_manager.chrome import ChromeDriverManager


# # ============================================================
# # í¬ë¡¤ë§ ì„¤ì •
# # ============================================================
# START_URL = "https://www.gachon.ac.kr/kor/3120/subview.do"
# DOMAIN = "www.gachon.ac.kr"

# DATA_DIR = "data"
# HTML_DIR = os.path.join(DATA_DIR, "html")
# TEXT_DIR = os.path.join(DATA_DIR, "text")
# FILE_DIR = os.path.join(DATA_DIR, "files")
# FILE_TEXT_DIR = os.path.join(DATA_DIR, "file_text")

# os.makedirs(HTML_DIR, exist_ok=True)
# os.makedirs(TEXT_DIR, exist_ok=True)
# os.makedirs(FILE_DIR, exist_ok=True)
# os.makedirs(FILE_TEXT_DIR, exist_ok=True)

# VISITED_FILE = "data/visited.txt"
# METADATA_FILE = "data/metadata.jsonl"
# QUEUE_FILE = "data/queue.txt"


# # ============================================================
# # DB ì„¤ì •
# #   - í•„ìš”í•˜ë©´ BASE_DIR / DB_PATH ìˆ˜ì •í•´ì„œ ì‚¬ìš©
# # ============================================================
# BASE_DIR = Path("/Users/kaia/Desktop/í•™êµ/Pp")
# BASE_DIR.mkdir(parents=True, exist_ok=True)
# DB_PATH = BASE_DIR / "gachon_raw_test.db"


# # ============================================================
# # íŒŒì¼ í™•ì¥ì + ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´
# # ============================================================
# FILE_EXTS = [
#     ".pdf", ".hwp", ".hwpx",
#     ".doc", ".docx",
#     ".xls", ".xlsx",
#     ".ppt", ".pptx",
#     ".zip"
# ]

# DOWNLOAD_URL_PATTERNS = [
#     "download.do",
# ]

# # ì¶”ê°€ ì°¨ë‹¨ íŒ¨í„´: synap ë·°ì–´ + ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
# BLOCK_PATTERNS = [
#     "synap",
#     "synapview.do",
#     "synapviewer",

#     "/eng/",
#     "/english/",
#     "/chi/",
#     "/chn/",
#     "/china/",
# ]


# # ============================================================
# # DB ì´ˆê¸°í™”
# # ============================================================
# def init_db():
#     conn = sqlite3.connect(DB_PATH)
#     cur = conn.cursor()
#     cur.execute(
#         """
#         CREATE TABLE IF NOT EXISTS documents (
#             id           INTEGER PRIMARY KEY AUTOINCREMENT,
#             url          TEXT NOT NULL,
#             source_type  TEXT,
#             title        TEXT,
#             raw_text     TEXT NOT NULL,
#             clean_text   TEXT,
#             meta_json    TEXT,
#             crawled_at   TEXT,
#             processed_at TEXT
#         );
#         """
#     )
#     conn.commit()
#     conn.close()
#     print(f"[INFO] DB ì´ˆê¸°í™” ì™„ë£Œ: {DB_PATH}")


# # ============================================================
# # URL í•„í„°ë§ í•¨ìˆ˜
# # ============================================================
# def is_file(url: str) -> bool:
#     u = url.lower()

#     if any(u.endswith(ext) for ext in FILE_EXTS):
#         return True

#     if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
#         return True

#     return False


# def is_download_url(url: str) -> bool:
#     u = url.lower()
#     return any(pat in u for pat in DOWNLOAD_URL_PATTERNS)


# def is_blocked_url(url: str) -> bool:
#     """HTML ë°©ë¬¸ì—ì„œ ì œì™¸í•´ì•¼ í•  URL"""
#     u = url.lower()

#     # 1) download.do â†’ HTML ë°©ë¬¸ ê¸ˆì§€
#     if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
#         return True

#     # 2) synap ë¬¸ì„œë·°ì–´ í˜ì´ì§€
#     if any(pat in u for pat in ["synap", "synapview", "synapviewer"]):
#         return True

#     # 3) ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
#     if any(pat in u for pat in ["/eng/", "/english/", "/chi/", "/chn/", "/china/"]):
#         return True

#     return False


# # ============================================================
# # Selenium ë“œë¼ì´ë²„ ìƒì„± (1ë²ˆ ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜)
# # ============================================================
# def create_driver():
#     chrome_options = Options()

#     # headless ê¸ˆì§€ (í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œí•´ì„œ ì‚¬ìš©)
#     # chrome_options.add_argument("--headless=new")

#     chrome_options.add_argument("--disable-gpu")
#     chrome_options.add_argument("--no-sandbox")
#     chrome_options.add_argument("--disable-dev-shm-usage")
#     chrome_options.add_argument("--disable-notifications")
#     chrome_options.add_argument("--disable-popup-blocking")

#     chrome_options.add_argument(
#         "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
#         "AppleWebKit/537.36 (KHTML, like Gecko) "
#         "Chrome/120.0.0.0 Safari/537.36"
#     )

#     # PDF ë·°ì–´ ë„ê³  ìë™ ë‹¤ìš´ë¡œë“œ
#     prefs = {
#         "download.default_directory": os.path.abspath(FILE_DIR),
#         "download.prompt_for_download": False,
#         "download.directory_upgrade": True,
#         "plugins.always_open_pdf_externally": True,
#     }
#     chrome_options.add_experimental_option("prefs", prefs)

#     service = Service(ChromeDriverManager().install())
#     driver = webdriver.Chrome(service=service, options=chrome_options)

#     driver.set_page_load_timeout(10)
#     driver.set_script_timeout(10)

#     return driver


# # ============================================================
# # ìœ í‹¸ í•¨ìˆ˜
# # ============================================================
# def canonicalize(url):
#     url, _ = urldefrag(url)
#     return url


# def load_visited():
#     if os.path.exists(VISITED_FILE):
#         with open(VISITED_FILE) as f:
#             return set(line.strip() for line in f)
#     return set()


# def save_visited(url):
#     with open(VISITED_FILE, "a") as f:
#         f.write(url + "\n")


# def load_queue():
#     if os.path.exists(QUEUE_FILE):
#         with open(QUEUE_FILE) as f:
#             return [line.strip() for line in f if line.strip()]
#     return []


# def save_queue(queue):
#     with open(QUEUE_FILE, "w") as f:
#         f.write("\n".join(queue))


# def save_text(content, idx):
#     """í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
#     fpath = os.path.join(TEXT_DIR, f"{idx:05d}.txt")
#     with open(fpath, "w", encoding="utf-8") as f:
#         f.write(content)
#     return fpath


# # ============================================================
# # HTML â†’ "ë³¸ë¬¸" í…ìŠ¤íŠ¸ ì¶”ì¶œ (2ë²ˆ ìŠ¤í¬ë¦½íŠ¸ì˜ extract_main_text ì‚¬ìš©)
# # ============================================================
# def extract_main_text(html: str) -> str:
#     soup = BeautifulSoup(html, "html.parser")

#     # 1) í™•ì‹¤íˆ í•„ìš” ì—†ëŠ” íƒœê·¸ ì œê±°
#     for tag in soup(["script", "style", "noscript"]):
#         tag.decompose()

#     # 2) header/footer íƒœê·¸ ì§ì ‘ ì œê±°
#     for tag in soup.find_all(["header", "footer"]):
#         tag.decompose()

#     # 3) ì‚¬ì´íŠ¸ì—ì„œ ìì£¼ ì“°ëŠ” í—¤ë”/í‘¸í„° CSS ID/class ì œê±°
#     header_footer_selectors = [
#         "#header",          # ê°€ì²œëŒ€ ìµœìƒë‹¨ í—¤ë”
#         "#footer",          # í˜ì´ì§€ í‘¸í„°
#         ".header",          # ê³µí†µ header class
#         ".footer",          # ê³µí†µ footer class
#         ".gnb",             # ê¸€ë¡œë²Œ ë„¤ë¹„ê²Œì´ì…˜ ë°”
#         "#gnb",             # ë©”ë‰´
#         ".logo",            # ìƒë‹¨ ë¡œê³  ì˜ì—­
#         ".site-map",        # ì‚¬ì´íŠ¸ë§µ
#         ".sub-visual",      # ì„œë¸Œ ë¹„ì£¼ì–¼ ì´ë¯¸ì§€
#         ".top-banner",      # ìƒë‹¨ ë°°ë„ˆ
#         ".bottom-banner",   # í•˜ë‹¨ ë°°ë„ˆ
#         ".quick-menu",      # í€µë©”ë‰´
#         ".breadcrumb",      # í˜„ì¬ ìœ„ì¹˜ breadcrumb
#         ".location",        # ìœ„ì¹˜ í‘œì‹œ UI
#         ".nav",             # ë„¤ë¹„ê²Œì´ì…˜
#         ".menu",            # ë©”ë‰´ ì „ì²´
#         ".wrap-header",
#         ".wrap-footer",
#         "#wrap-header",
#         "#wrap-footer",
#         ".sns-area",
#         ".top-menu",
#     ]

#     for selector in header_footer_selectors:
#         for tag in soup.select(selector):
#             tag.decompose()

#     # 4) ê°€ì²œëŒ€ ë³¸ë¬¸ ì˜ì—­ (#content, #contents, .content-wrapper ë“±)
#     main_candidates = [
#         "#content",
#         "#contents",
#         ".content",
#         ".contents",
#         ".content-area",
#         ".content-wrapper",
#         ".sub-content",
#         ".sub-contents",
#         ".article",
#         ".board-view",     # ê²Œì‹œíŒ ë³¸ë¬¸
#         "#container",
#     ]

#     main = None
#     for sel in main_candidates:
#         area = soup.select_one(sel)
#         if area:
#             main = area
#             break

#     if main is None:
#         main = soup  # fallback

#     text = main.get_text(separator="\n")
#     lines = [ln.strip() for ln in text.splitlines() if ln.strip()]

#     return "\n".join(lines)


# # ============================================================
# # ì•ˆì „ GET
# # ============================================================
# def safe_get(driver, url):
#     try:
#         driver.get(url)
#         # JS ë Œë”ë§ ì—¬ìœ 
#         time.sleep(2)

#         try:
#             alert = driver.switch_to.alert
#             print("[ALERT FOUND]", alert.text)
#             alert.accept()
#             return False
#         except Exception:
#             pass

#         return True

#     except Exception as e:
#         print(f"[ERROR safe_get] {e}")
#         return False


# # ============================================================
# # ë©”ì¸ í¬ë¡¤ëŸ¬ (DB ì €ì¥ í¬í•¨)
# # ============================================================
# def crawl():
#     driver = create_driver()

#     visited = load_visited()
#     queue = load_queue()

#     start = canonicalize(START_URL)
#     if not queue:
#         queue.append(start)

#     seen = set(visited) | set(queue)
#     idx = len(visited)

#     # DB ì—°ê²°
#     conn = sqlite3.connect(DB_PATH)
#     cur = conn.cursor()

#     print("[INFO] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
#     print(f"[INFO] ë°©ë¬¸ ì™„ë£Œ {len(visited)}ê°œ")
#     print(f"[INFO] ëŒ€ê¸° ì¤‘ {len(queue)}ê°œ\n")

#     try:
#         while queue:
#             url = queue.pop(0)
#             idx += 1

#             print(f"\n[{idx}] GET {url}")

#             # ğŸ”¥ HTML ì°¨ë‹¨ íŒ¨í„´
#             if is_blocked_url(url):
#                 print("    [SKIP] ì°¨ë‹¨ëœ URL:", url)
#                 visited.add(url)
#                 save_visited(url)
#                 save_queue(queue)
#                 continue

#             # ğŸ”¥ download.do ìì²´ëŠ” HTML ë°©ë¬¸ ê¸ˆì§€
#             if is_download_url(url):
#                 print("    [SKIP] download URL:", url)
#                 visited.add(url)
#                 save_visited(url)
#                 save_queue(queue)
#                 continue

#             # ë°©ë¬¸
#             success = safe_get(driver, url)
#             if not success:
#                 print("[SKIP] alert/ì˜¤ë¥˜:", url)
#                 visited.add(url)
#                 save_visited(url)
#                 save_queue(queue)
#                 continue

#             html = driver.page_source
#             soup = BeautifulSoup(html, "html.parser")
#             title = soup.title.get_text(strip=True) if soup.title else ""

#             # ============================
#             # 1) ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (DBìš©)
#             # ============================
#             main_text = extract_main_text(html)

#             # ============================
#             # 2) í…ìŠ¤íŠ¸ íŒŒì¼ë¡œë„ ì €ì¥ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
#             # ============================
#             text_file_path = save_text(main_text, idx)

#             # ============================
#             # 3) JSONL metadata ì €ì¥
#             # ============================
#             metadata = {
#                 "id": idx,
#                 "type": "html",
#                 "url": url,
#                 "text": text_file_path,
#                 "title": title,
#                 "timestamp": time.time()
#             }
#             with open(METADATA_FILE, "a", encoding="utf-8") as f:
#                 f.write(json.dumps(metadata, ensure_ascii=False) + "\n")

#             # ============================
#             # 4) DBì— INSERT
#             # ============================
#             cur.execute(
#                 """
#                 INSERT INTO documents
#                     (url, source_type, title, raw_text, clean_text, meta_json, crawled_at, processed_at)
#                 VALUES (?, ?, ?, ?, ?, ?, ?, ?)
#                 """,
#                 (
#                     url,
#                     "html",
#                     title,
#                     main_text,
#                     None,  # clean_text ì•„ì§ ì—†ìŒ
#                     json.dumps(metadata, ensure_ascii=False),
#                     datetime.now().isoformat(timespec="seconds"),
#                     None,
#                 ),
#             )
#             conn.commit()

#             # -------------------------
#             # ë§í¬ íƒìƒ‰
#             # -------------------------
#             for a in soup.find_all("a", href=True):
#                 href = a["href"].strip()

#                 if href.startswith("javascript:") or href.startswith("mailto:"):
#                     continue

#                 next_url = canonicalize(urljoin(url, href))

#                 # ğŸ”¥ íŒŒì¼ ì²˜ë¦¬
#                 if is_file(next_url):
#                     print(f"    [FILE LINK] {next_url}")

#                     ext = os.path.splitext(next_url)[1].lower()
#                     if ext == "":
#                         ext = ".temp"

#                     try:
#                         driver.execute_script("window.open(arguments[0]);", next_url)
#                     except Exception:
#                         pass

#                     file_meta = {
#                         "id": f"file-{idx}-{len(os.listdir(FILE_DIR)) + 1}",
#                         "type": f"file:{ext}",
#                         "url": next_url,
#                         "ref_page_url": url,
#                         "ref_page_id": idx,
#                         "file_path": None,
#                         "timestamp": time.time()
#                     }
#                     with open(METADATA_FILE, "a", encoding="utf-8") as f:
#                         f.write(json.dumps(file_meta, ensure_ascii=False) + "\n")
#                     continue

#                 # ğŸ”¥ HTML ì°¨ë‹¨ URLì€ ë°©ë¬¸ ê¸ˆì§€
#                 if is_blocked_url(next_url):
#                     continue

#                 # ğŸ”¥ ì™¸ë¶€ ë„ë©”ì¸ ê¸ˆì§€
#                 if urlparse(next_url).netloc != DOMAIN:
#                     continue

#                 if next_url not in seen:
#                     seen.add(next_url)
#                     queue.append(next_url)
#                     save_queue(queue)

#             visited.add(url)
#             save_visited(url)
#             save_queue(queue)

#     finally:
#         conn.close()
#         driver.quit()
#         print("\n[DONE] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ + DB ì €ì¥ ì™„ë£Œ")


# # ============================================================
# # ì‹¤í–‰
# # ============================================================
# def main():
#     init_db()
#     crawl()


# if __name__ == "__main__":
#     main()


# path = "/home/t25315/data/yo_txt/â˜… 2024 ìš”ëŒ(ì´ëŒ ë° êµì–‘) 2024.05.24.txt"
# f = open(path, "r", encoding="cp949")
# text = f.read()
# f.close()
# print(text)

# convert_encoding.py

src_path = "/home/t25315/data/yo_txt/2020_ê°€ì²œëŒ€í•™êµ_ìš”ëŒ(ì´ëŒ).txt"          # ì›ë³¸ íŒŒì¼ (CP949)
dst_path = "/home/t25315/data/yo_txt/yo_txt_encoding/2020_ìš”ëŒ(ì´ëŒ)_raw_utf8.txt"     # ë³€í™˜í•´ì„œ ì €ì¥í•  íŒŒì¼ (UTF-8)

with open(src_path, "r", encoding="cp949", errors="ignore") as f:
    text = f.read()

with open(dst_path, "w", encoding="utf-8") as f:
    f.write(text)

print("ë³€í™˜ ì™„ë£Œ:", dst_path)
