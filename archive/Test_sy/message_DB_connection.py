#!/usr/bin/env python
# -*- coding: utf-8 -*-

from datetime import datetime
import pymysql
from db_config import get_conn

import sys
import os
import time
import json
from urllib.parse import urljoin, urlparse, urldefrag
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import UnexpectedAlertPresentException
from webdriver_manager.chrome import ChromeDriverManager

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_DIR)

from Pproject.Test_sy.raw_db_selenium import extract_main_text


# ============================================================
# DB ì—°ê²°
# ============================================================

def insert_raw_document(conn, url, title, raw_text, meta_dict=None):
    """
    í•œ í˜ì´ì§€ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ documents í…Œì´ë¸”ì— ì €ì¥.
    ì´ë¯¸ ê°™ì€ urlì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸.
    """
    meta_json = json.dumps(meta_dict, ensure_ascii=False) if meta_dict else None

    sql = """
    INSERT INTO documents (url, source_type, title, raw_text, meta_json, crawled_at)
    VALUES (%s, %s, %s, %s, %s, %s)
    ON DUPLICATE KEY UPDATE
        title      = VALUES(title),
        raw_text   = VALUES(raw_text),
        meta_json  = VALUES(meta_json),
        crawled_at = VALUES(crawled_at)
    """
    with conn.cursor() as cur:
        cur.execute(sql, (
            url,
            "html",   # ì§€ê¸ˆì€ HTML í˜ì´ì§€ë¼ì„œ ê³ ì •
            title,
            raw_text,
            meta_json,
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        ))
    conn.commit()

# ============================================================
# ì„¤ì •
# ============================================================
START_URL = "https://www.gachon.ac.kr/kor/3120/subview.do"
DOMAIN = "www.gachon.ac.kr"

DATA_DIR = "data"
HTML_DIR = os.path.join(DATA_DIR, "html")
TEXT_DIR = os.path.join(DATA_DIR, "text")
FILE_DIR = os.path.join(DATA_DIR, "files")
FILE_TEXT_DIR = os.path.join(DATA_DIR, "file_text")

os.makedirs(HTML_DIR, exist_ok=True)
os.makedirs(TEXT_DIR, exist_ok=True)
os.makedirs(FILE_DIR, exist_ok=True)
os.makedirs(FILE_TEXT_DIR, exist_ok=True)

VISITED_FILE = "data/visited.txt"
METADATA_FILE = "data/metadata.jsonl"
QUEUE_FILE = "data/queue.txt"


# ============================================================
# íŒŒì¼ í™•ì¥ì + ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´
# ============================================================
FILE_EXTS = [
    ".pdf", ".hwp", ".hwpx",
    ".doc", ".docx",
    ".xls", ".xlsx",
    ".ppt", ".pptx",
    ".zip"
]

DOWNLOAD_URL_PATTERNS = [
    "download.do",
]

# ì¶”ê°€ ì°¨ë‹¨ íŒ¨í„´: synap ë·°ì–´ + ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
BLOCK_PATTERNS = [
    "synap",
    "synapview.do",
    "synapviewer",

    "/eng/",
    "/english/",
    "/chi/",
    "/chn/",
    "/china/",
]


def is_file(url: str) -> bool:
    u = url.lower()

    if any(u.endswith(ext) for ext in FILE_EXTS):
        return True

    if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
        return True

    return False


def is_download_url(url: str) -> bool:
    u = url.lower()
    return any(pat in u for pat in DOWNLOAD_URL_PATTERNS)


def is_blocked_url(url: str) -> bool:
    """HTML ë°©ë¬¸ì—ì„œ ì œì™¸í•´ì•¼ í•  URL"""
    u = url.lower()

    # 1) download.do â†’ HTML ë°©ë¬¸ ê¸ˆì§€
    if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
        return True

    # 2) synap ë¬¸ì„œë·°ì–´ í˜ì´ì§€
    if any(pat in u for pat in ["synap", "synapview", "synapviewer"]):
        return True

    # 3) ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
    if any(pat in u for pat in ["/eng/", "/english/", "/chi/", "/chn/", "/china/"]):
        return True

    return False


# ============================================================
# ë“œë¼ì´ë²„ ìƒì„±
# ============================================================
def create_driver():
    chrome_options = Options()

    # headless ê¸ˆì§€
    # chrome_options.add_argument("--headless=new")

    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--disable-popup-blocking")

    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )

    # PDF ë·°ì–´ ë„ê³  ìë™ ë‹¤ìš´ë¡œë“œ
    prefs = {
        "download.default_directory": os.path.abspath(FILE_DIR),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "plugins.always_open_pdf_externally": True,
    }
    chrome_options.add_experimental_option("prefs", prefs)

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    driver.set_page_load_timeout(4)
    driver.set_script_timeout(4)

    return driver


# ============================================================
# ìœ í‹¸ í•¨ìˆ˜
# ============================================================
def canonicalize(url):
    url, _ = urldefrag(url)
    return url


def load_visited():
    if os.path.exists(VISITED_FILE):
        with open(VISITED_FILE) as f:
            return set(line.strip() for line in f)
    return set()


def save_visited(url):
    with open(VISITED_FILE, "a") as f:
        f.write(url + "\n")


def load_queue():
    if os.path.exists(QUEUE_FILE):
        with open(QUEUE_FILE) as f:
            return [line.strip() for line in f if line.strip()]
    return []


def save_queue(queue):
    with open(QUEUE_FILE, "w") as f:
        f.write("\n".join(queue))


def html_to_text(html):

    return extract_main_text(html)


def save_text(content, idx):
    fpath = os.path.join(TEXT_DIR, f"{idx:05d}.txt")
    with open(fpath, "w", encoding="utf-8") as f:
        f.write(content)
    return fpath


# ============================================================
# ì•ˆì „ GET
# ============================================================
def safe_get(driver, url):
    try:
        driver.get(url)
        time.sleep(0.1)

        try:
            alert = driver.switch_to.alert
            print("[ALERT FOUND]", alert.text)
            alert.accept()
            return False
        except:
            pass

        return True

    except Exception as e:
        print(f"[ERROR safe_get] {e}")
        return False


# ============================================================
# ë©”ì¸ í¬ë¡¤ëŸ¬
# ============================================================
def crawl():
    driver = create_driver()
    conn = get_conn()

    visited = load_visited()
    queue = load_queue()

    start = canonicalize(START_URL)
    if not queue:
        queue.append(start)

    seen = set(visited) | set(queue)
    idx = len(visited)

    print("[INFO] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
    print(f"[INFO] ë°©ë¬¸ ì™„ë£Œ {len(visited)}ê°œ")
    print(f"[INFO] ëŒ€ê¸° ì¤‘ {len(queue)}ê°œ\n")

    while queue:
        url = queue.pop(0)
        idx += 1

        print(f"\n[{idx}] GET {url}")

        # ğŸ”¥ HTML ì°¨ë‹¨ íŒ¨í„´
        if is_blocked_url(url):
            print("    [SKIP] ì°¨ë‹¨ëœ URL:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        # ğŸ”¥ download.do ìì²´ëŠ” HTML ë°©ë¬¸ ê¸ˆì§€
        if is_download_url(url):
            print("    [SKIP] download URL:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        # ë°©ë¬¸
        success = safe_get(driver, url)
        if not success:
            print("[SKIP] alert/ì˜¤ë¥˜:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        html = driver.page_source

        # HTML â†’ TEXT ì €ì¥
        html_text = html_to_text(html)
        html_text_path = save_text(html_text, idx)

        # metadata ì €ì¥
        soup = BeautifulSoup(html, "html.parser")
        title = soup.title.string if soup.title else ""

        metadata = {
            "id": idx,
            "type": "html",
            "url": url,
            "text": html_text_path,
            "title": title,
            "timestamp": time.time()
        }
        with open(METADATA_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(metadata, ensure_ascii=False) + "\n")

        page_meta = {
            "local_text_path": html_text_path,
            "crawler_id": idx,
        }
        insert_raw_document(conn, url, title, html_text, page_meta)

        

        # -------------------------
        # ë§í¬ íƒìƒ‰
        # -------------------------
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()

            if href.startswith("javascript:") or href.startswith("mailto:"):
                continue

            next_url = canonicalize(urljoin(url, href))

            # ğŸ”¥ íŒŒì¼ ì²˜ë¦¬
            if is_file(next_url):
                print(f"    [FILE LINK] {next_url}")

                ext = os.path.splitext(next_url)[1].lower()
                if ext == "":
                    ext = ".temp"

                try:
                    driver.execute_script("window.open(arguments[0]);", next_url)
                except:
                    pass

                file_meta = {
                    "id": f"file-{idx}-{len(os.listdir(FILE_DIR))+1}",
                    "type": f"file:{ext}",
                    "url": next_url,
                    "ref_page_url": url,
                    "ref_page_id": idx,
                    "file_path": None,
                    "timestamp": time.time()
                }
                with open(METADATA_FILE, "a", encoding="utf-8") as f:
                    f.write(json.dumps(file_meta, ensure_ascii=False) + "\n")
                continue

            # ğŸ”¥ HTML ì°¨ë‹¨ URLì€ ë°©ë¬¸ ê¸ˆì§€
            if is_blocked_url(next_url):
                continue

            # ğŸ”¥ ì™¸ë¶€ ë„ë©”ì¸ ê¸ˆì§€
            if urlparse(next_url).netloc != DOMAIN:
                continue

            if next_url not in seen:
                seen.add(next_url)
                queue.append(next_url)
                save_queue(queue)

        visited.add(url)
        save_visited(url)
        save_queue(queue)

    driver.quit()
    conn.close()
    print("\n[DONE] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ")


# ì‹¤í–‰
if __name__ == "__main__":
    crawl()