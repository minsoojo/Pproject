## í¬ë¡¤ë§ ìˆ˜í–‰ ì½”ë“œ

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import time
import json
from urllib.parse import urljoin, urlparse, urldefrag
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import UnexpectedAlertPresentException
from webdriver_manager.chrome import ChromeDriverManager


# ============================================================
# ì„¤ì •
# ============================================================
START_URL = "https://www.gachon.ac.kr/kor/3120/subview.do"
DOMAIN = "www.gachon.ac.kr"

DATA_DIR = "data"
HTML_DIR = os.path.join(DATA_DIR, "html")
TEXT_DIR = os.path.join(DATA_DIR, "text")
FILE_DIR = os.path.join(DATA_DIR, "files")
FILE_TEXT_DIR = os.path.join(DATA_DIR, "file_text")

os.makedirs(HTML_DIR, exist_ok=True)
os.makedirs(TEXT_DIR, exist_ok=True)
os.makedirs(FILE_DIR, exist_ok=True)
os.makedirs(FILE_TEXT_DIR, exist_ok=True)

VISITED_FILE = "data/visited.txt"
METADATA_FILE = "data/metadata.jsonl"
QUEUE_FILE = "data/queue.txt"


# ============================================================
# íŒŒì¼ í™•ì¥ì + ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´
# ============================================================
FILE_EXTS = [
    ".pdf", ".hwp", ".hwpx",
    ".doc", ".docx",
    ".xls", ".xlsx",
    ".ppt", ".pptx",
    ".zip"
]

DOWNLOAD_URL_PATTERNS = [
    "download.do",
]

# ì¶”ê°€ ì°¨ë‹¨ íŒ¨í„´: synap ë·°ì–´ + ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
BLOCK_PATTERNS = [
    # "synap",
    "synapview.do",
    # "synapviewer",

    "/eng/",
    "/english/",
    "/chi/",
    "/chn/",
    "/china/",
]


def is_file(url: str) -> bool:
    u = url.lower()

    if any(u.endswith(ext) for ext in FILE_EXTS):
        return True

    if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
        return True

    return False


def is_download_url(url: str) -> bool:
    u = url.lower()
    return any(pat in u for pat in DOWNLOAD_URL_PATTERNS)


def is_blocked_url(url: str) -> bool:
    """HTML ë°©ë¬¸ì—ì„œ ì œì™¸í•´ì•¼ í•  URL"""
    u = url.lower()

    # 1) download.do â†’ HTML ë°©ë¬¸ ê¸ˆì§€
    if any(pat in u for pat in DOWNLOAD_URL_PATTERNS):
        return True

    # 2) synap ë¬¸ì„œë·°ì–´ í˜ì´ì§€
    if any(pat in u for pat in ["synap", "synapview", "synapviewer"]):
        return True

    # 3) ì˜ì–´/ì¤‘êµ­ì–´ ì‚¬ì´íŠ¸
    if any(pat in u for pat in ["/eng/", "/english/", "/chi/", "/chn/", "/china/"]):
        return True

    return False


# ============================================================
# ë“œë¼ì´ë²„ ìƒì„±
# ============================================================
def create_driver():
    chrome_options = Options()

    # headless ê¸ˆì§€
    chrome_options.add_argument("--headless=new")

    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-notifications")
    chrome_options.add_argument("--disable-popup-blocking")

    chrome_options.binary_location = "/home/t25315/chromium-portable/chrome-linux/chrome"
    

    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )

    # PDF ë·°ì–´ ë„ê³  ìë™ ë‹¤ìš´ë¡œë“œ
    prefs = {
        "download.default_directory": os.path.abspath(FILE_DIR),
        "download.prompt_for_download": False,
        "download.directory_upgrade": True,
        "plugins.always_open_pdf_externally": True,
    }
    chrome_options.add_experimental_option("prefs", prefs)

    # service = Service(ChromeDriverManager().install())
    service = Service("/home/t25315/chromium-portable/chromedriver_linux64/chromedriver")
    driver = webdriver.Chrome(service=service, options=chrome_options)

    driver.set_page_load_timeout(14)
    driver.set_script_timeout(14)

    return driver


# ============================================================
# ìœ í‹¸ í•¨ìˆ˜
# ============================================================
# def canonicalize(url):
#     url, _ = urldefrag(url)
#     return url
import re
from urllib.parse import urlparse, urlunparse, urldefrag
def canonicalize(url: str) -> str:
    # 1) fragment ì œê±° (# ë’¤ ì œê±°)
    url, _ = urldefrag(url)

    # 2) ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì •ë¦¬ (:// ì œì™¸)
    url = re.sub(r'(?<!:)//+', '/', url)

    # 3) '..' ê°™ì€ ì´ìƒí•œ dot íŒ¨í„´ ì •ë¦¬
    url = url.replace("..do", ".do")
    url = url.replace("..html", ".html")
    url = url.replace("..php", ".php")
    
    # 4) ë§ˆì§€ë§‰ì— '/' ë¶™ì§€ ì•Šê²Œ
    if url.endswith("/"):
        url = url[:-1]

    # 5) URL êµ¬ì¡° ë‹¤ì‹œ ì¡°í•©
    parsed = urlparse(url)
    normalized = urlunparse(parsed)

    return normalized


def load_visited():
    if os.path.exists(VISITED_FILE):
        with open(VISITED_FILE) as f:
            return set(line.strip() for line in f)
    return set()


def save_visited(url):
    with open(VISITED_FILE, "a") as f:
        f.write(url + "\n")


def load_queue():
    if os.path.exists(QUEUE_FILE):
        with open(QUEUE_FILE) as f:
            return [line.strip() for line in f if line.strip()]
    return []


def save_queue(queue):
    with open(QUEUE_FILE, "w") as f:
        f.write("\n".join(queue))


# def html_to_text(html):
#     soup = BeautifulSoup(html, "html.parser")
#     for tag in soup(["script", "style", "noscript", "footer", "nav"]):
#         tag.decompose()

#     text = soup.get_text(separator="\n")
#     return "\n".join(x.strip() for x in text.splitlines() if x.strip())


def html_to_text(html: str) -> str:
    # soup = BeautifulSoup(html, "html.parser")
    soup = BeautifulSoup(html, "lxml")

    # 1) í™•ì‹¤íˆ í•„ìš” ì—†ëŠ” íƒœê·¸ ì œê±°
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # 2) header/footer íƒœê·¸ ì§ì ‘ ì œê±°
    for tag in soup.find_all(["header", "footer"]):
        tag.decompose()

    # 3) ì‚¬ì´íŠ¸ì—ì„œ ìì£¼ ì“°ëŠ” í—¤ë”/í‘¸í„° CSS ID/class ì œê±°
    header_footer_selectors = [
        "#header",          # ê°€ì²œëŒ€ ìµœìƒë‹¨ í—¤ë”
        "#footer",          # í˜ì´ì§€ í‘¸í„°
        ".header",          # ê³µí†µ header class
        ".footer",          # ê³µí†µ footer class
        ".gnb",             # ê¸€ë¡œë²Œ ë„¤ë¹„ê²Œì´ì…˜ ë°”
        "#gnb",             # ë©”ë‰´
        ".logo",            # ìƒë‹¨ ë¡œê³  ì˜ì—­
        ".site-map",        # ì‚¬ì´íŠ¸ë§µ
        ".sub-visual",      # ì„œë¸Œ ë¹„ì£¼ì–¼ ì´ë¯¸ì§€
        ".top-banner",      # ìƒë‹¨ ë°°ë„ˆ
        ".bottom-banner",   # í•˜ë‹¨ ë°°ë„ˆ
        ".quick-menu",      # í€µë©”ë‰´
        ".breadcrumb",      # í˜„ì¬ ìœ„ì¹˜ breadcrumb
        ".location",        # ìœ„ì¹˜ í‘œì‹œ UI
        ".nav",             # ë„¤ë¹„ê²Œì´ì…˜
        ".menu",            # ë©”ë‰´ ì „ì²´
        ".wrap-header",
        ".wrap-footer",
        "#wrap-header",
        "#wrap-footer",
        ".sns-area",
        ".top-menu",
    ]

    # for selector in header_footer_selectors:
    #     for tag in soup.select(selector):
    #         tag.decompose()
    for selector in header_footer_selectors:
        try:
            for tag in soup.select(selector):
                tag.decompose()
        except Exception:
            continue

    # 4) ê°€ì²œëŒ€ ë³¸ë¬¸ ì˜ì—­(#content, #contents, .content-wrapper ë“±)
    main_candidates = [
        "#content",
        "#contents",
        ".content",
        ".contents",
        ".content-area",
        ".content-wrapper",
        ".sub-content",
        ".sub-contents",
        ".article",
        ".board-view",     # ê²Œì‹œíŒ ë³¸ë¬¸
        "#container",
    ]

    main = None

    for sel in main_candidates:
        area = soup.select_one(sel)
        if area:
            main = area
            break   
        
    if main is None:
        candidates = soup.find_all(["article", "section"])
        if candidates:
            main = max(
                candidates,
                key=lambda c: len(c.get_text(strip=True))
            )

    if main is None:  
        main = soup  # fallback

    markdown_tables = []

    tables = main.find_all("table")
    for t in tables:
        rows = t.find_all("tr")
        table_lines = []
        for i, row in enumerate(rows):
            cells = row.find_all(["th", "td"])
            cell_texts = [c.get_text(strip=True) for c in cells]
            # ë¹ˆ í–‰ì€ ìŠ¤í‚µ
            if not any(cell_texts):
                continue

            # ë³¸ë¬¸
            table_lines.append("| " + " | ".join(cell_texts) + " |")

            # ì²« í–‰ì„ í—¤ë”ë¡œ ë³´ê³  êµ¬ë¶„ì„  ì¶”ê°€
            if i == 0:
                table_lines.append("| " + " | ".join(["---"] * len(cell_texts)) + " |")

        if table_lines:
            markdown_tables.append("\n".join(table_lines))

    # 5-2) table ì„ DOMì—ì„œ ì œê±°í•´ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì™€ ì¤‘ë³µ ë°©ì§€
    for t in tables:
        t.decompose()

    # 6) ë³¸ë¬¸ í…ìŠ¤íŠ¸
    text = main.get_text(separator="\n")
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    body_text = "\n".join(lines)

    # 7) í…Œì´ë¸”ì„ markdownìœ¼ë¡œ ë¶™ì—¬ì¤Œ
    if markdown_tables:
        return body_text + "\n\n" + "\n\n".join(markdown_tables)
    return body_text    

def save_text(content, idx):
    fpath = os.path.join(TEXT_DIR, f"{idx:05d}.txt")
    with open(fpath, "w", encoding="utf-8") as f:
        f.write(content)
    return fpath


# ============================================================
# ì•ˆì „ GET
# ============================================================
def safe_get(driver, url):
    try:
        driver.get(url)
        time.sleep(0.1)

        try:
            alert = driver.switch_to.alert
            print("[ALERT FOUND]", alert.text)
            alert.accept()
            return False
        except:
            pass

        return True

    except Exception as e:
        print(f"[ERROR safe_get] {e}")
        return False
    
# ============================================================
# wait_for_download
# ============================================================

def wait_for_download(dir_path, timeout=15):
    """
    ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ë¥¼ ê°ì‹œí•˜ì—¬ ìƒˆ íŒŒì¼ì´ ìƒì„±ë˜ë©´ í•´ë‹¹ ê²½ë¡œë¥¼ ë°˜í™˜
    """
    before = set(os.listdir(dir_path))
    for _ in range(timeout * 10):  # 0.1ì´ˆ ê°„ê²©
        time.sleep(0.1)
        after = set(os.listdir(dir_path))
        new_files = after - before
        if new_files:
            return os.path.join(dir_path, list(new_files)[0])
    return None
# ============================================================
# ë©”ì¸ í¬ë¡¤ëŸ¬
# ============================================================
def crawl():
    driver = create_driver()

    visited = load_visited()
    queue = load_queue()
    
    start = canonicalize(START_URL)
    if not queue:
        queue.append(start)

    seen = set(visited) | set(queue)
    idx = len(visited)

    print("[INFO] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
    print(f"[INFO] ë°©ë¬¸ ì™„ë£Œ {len(visited)}ê°œ")
    print(f"[INFO] ëŒ€ê¸° ì¤‘ {len(queue)}ê°œ\n")

    while queue:
        url = queue.pop(0)
        idx += 1

        print(f"\n[{idx}] GET {url}")
        
        # ğŸ”¥ HTML ì°¨ë‹¨ íŒ¨í„´
        if is_blocked_url(url):
            print("    [SKIP] ì°¨ë‹¨ëœ URL:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        # ğŸ”¥ download.do ìì²´ëŠ” HTML ë°©ë¬¸ ê¸ˆì§€
        if is_download_url(url):
            print("    [SKIP] download URL:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        # ë°©ë¬¸
        success = safe_get(driver, url)
        if not success:
            print("[SKIP] alert/ì˜¤ë¥˜:", url)
            visited.add(url)
            save_visited(url)
            save_queue(queue)
            continue

        html = driver.page_source

        # HTML â†’ TEXT ì €ì¥
        html_text = html_to_text(html)
        html_text_path = save_text(html_text, idx)

        # metadata ì €ì¥
        soup = BeautifulSoup(html, "lxml")
        title = soup.title.string if soup.title else ""

        metadata = {
            "id": idx,
            "type": "html",
            "url": url,
            "text": html_text_path,
            "title": title,
            "timestamp": time.time()
        }
        with open(METADATA_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(metadata, ensure_ascii=False) + "\n")    

        # -------------------------
        # ë§í¬ íƒìƒ‰
        # -------------------------
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()

            if href.startswith("javascript:") or href.startswith("mailto:"):
                continue

            next_url = canonicalize(urljoin(url, href))

            if next_url.startswith("http://"):
                continue

            # ğŸ”¥ íŒŒì¼ ì²˜ë¦¬
            if is_file(next_url):
                print(f"    [FILE LINK] {next_url}")

                ext = os.path.splitext(next_url)[1].lower()
                if ext == "":
                    ext = ".temp"

                try:
                    driver.execute_script("window.open(arguments[0]);", next_url)
                except:
                    pass
                
                downloaded_path = wait_for_download(FILE_DIR)

                file_meta = {
                    "id": f"file-{idx}-{len(os.listdir(FILE_DIR))+1}",
                    "type": f"file:{ext}",
                    "url": next_url,
                    "ref_page_url": url,
                    "ref_page_id": idx,
                    "file_path": downloaded_path,
                    "timestamp": time.time()
                }
                with open(METADATA_FILE, "a", encoding="utf-8") as f:
                    f.write(json.dumps(file_meta, ensure_ascii=False) + "\n")
                continue

            # ğŸ”¥ HTML ì°¨ë‹¨ URLì€ ë°©ë¬¸ ê¸ˆì§€
            if is_blocked_url(next_url):
                continue

            # ğŸ”¥ ì™¸ë¶€ ë„ë©”ì¸ ê¸ˆì§€
            if urlparse(next_url).netloc != DOMAIN:
                continue

            if next_url not in seen:
                seen.add(next_url)
                queue.append(next_url)
                save_queue(queue)

        visited.add(url)
        save_visited(url)
        save_queue(queue)

    driver.quit()
    print("\n[DONE] ê°€ì²œëŒ€ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ")


# ì‹¤í–‰
if __name__ == "__main__":
    crawl()